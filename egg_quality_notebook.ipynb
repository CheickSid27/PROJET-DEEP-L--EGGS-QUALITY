{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8f607e9",
   "metadata": {},
   "source": [
    "# Good & Bad Eggs\n",
    "\n",
    "Ce notebook regroupe tout : prétraitement → features classiques → embeddings profonds → hybrides → réduction de dimension → classifieurs → évaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c3f4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Installer les paquets nécessaires (décommente si nécessaire)\n",
    "# !pip install timm xgboost scikit-image PyWavelets umap-learn joblib nbformat tensorflow\n",
    "\n",
    "import os, sys, time, json, math, random\n",
    "from pathlib import Path\n",
    "import sys\n",
    "sys.path.insert(0, \"numpy_for_cv2\")\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import cv2\n",
    "import pandas as pd\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Machine learning / DL\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "import timm\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score, accuracy_score, recall_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "import joblib\n",
    "\n",
    "# xgboost (import optional if installed)\n",
    "try:\n",
    "    import xgboost as xgb\n",
    "    XGBOOST_AVAILABLE = True\n",
    "except Exception as e:\n",
    "    XGBOOST_AVAILABLE = False\n",
    "\n",
    "print('torch:', torch.__version__, 'timm available:', 'timm' in sys.modules)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('Device:', device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f97cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# MODIFY THIS PATH to your local dataset root\n",
    "DATA_ROOT = Path(r'C:\\Users\\cheic\\Documents\\M2 IA MathInfo\\DeepL\\PROJET FINAL DPL')\n",
    "original_dir = DATA_ROOT / 'Original Images(Eggs)'\n",
    "aug_dir = DATA_ROOT / 'Augmented_Images(Eggs)'\n",
    "\n",
    "\n",
    "def gather_paths(base_dir):    \n",
    "    rows = []\n",
    "    if not base_dir.exists():\n",
    "        return rows\n",
    "    for label_dir in ['Good Eggs', 'Bad Eggs']:\n",
    "        p = base_dir / label_dir\n",
    "        if not p.exists():\n",
    "            continue\n",
    "        for img_path in p.glob('*'):\n",
    "            if img_path.suffix.lower() not in ['.jpg','.jpeg','.png','.bmp']:\n",
    "                continue\n",
    "            rows.append({'path': str(img_path), 'label': 'good' if 'Good' in label_dir else 'bad', 'source': base_dir.name})\n",
    "    return rows\n",
    "\n",
    "rows = []\n",
    "rows += gather_paths(original_dir)\n",
    "rows += gather_paths(aug_dir)\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "print('Total images found:', len(df))\n",
    "if len(df)>0:\n",
    "    df['label_enc'] = df['label'].map({'good':0,'bad':1})\n",
    "    display(df.head())\n",
    "else:\n",
    "    print('No images found. Check DATA_ROOT path.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09520f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preprocess_cv2(path_or_img, size=(224,224), apply_clahe=True):\n",
    "    # Accept path or numpy array (HWC BGR or RGB in 0..1)\n",
    "    if isinstance(path_or_img, str):\n",
    "        img = cv2.imread(path_or_img, cv2.IMREAD_COLOR)  # BGR uint8\n",
    "        if img is None:\n",
    "            raise FileNotFoundError(path_or_img)\n",
    "    else:\n",
    "        img = path_or_img.copy()\n",
    "        if img.dtype == np.float32 or img.dtype == np.float64:\n",
    "            img = (img * 255).astype('uint8')\n",
    "        if img.shape[-1] == 3:\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "    img = cv2.resize(img, size, interpolation=cv2.INTER_AREA)\n",
    "    lab = cv2.cvtColor(img, cv2.COLOR_BGR2LAB)\n",
    "    l, a, b = cv2.split(lab)\n",
    "    if apply_clahe:\n",
    "        clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
    "        l = clahe.apply(l)\n",
    "    lab = cv2.merge((l,a,b))\n",
    "    img = cv2.cvtColor(lab, cv2.COLOR_LAB2BGR)\n",
    "    img = cv2.medianBlur(img, 3)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB).astype('float32') / 255.0\n",
    "    return img\n",
    "\n",
    "# preview\n",
    "if 'df' in globals() and len(df) >= 6:\n",
    "    sample = df.sample(6, random_state=42).reset_index(drop=True)\n",
    "    plt.figure(figsize=(12,6))\n",
    "    for i,row in sample.iterrows():\n",
    "        try:\n",
    "            raw = cv2.imread(row['path'])\n",
    "            proc = preprocess_cv2(row['path'])\n",
    "            plt.subplot(2,6,i+1); plt.imshow(cv2.cvtColor(raw, cv2.COLOR_BGR2RGB)); plt.title(row['label']); plt.axis('off')\n",
    "            plt.subplot(2,6,i+7); plt.imshow(proc); plt.title('preprocessed'); plt.axis('off')\n",
    "        except Exception as e:\n",
    "            print('Preview error:', e)\n",
    "    plt.tight_layout()\n",
    "else:\n",
    "    print('Not enough images to preview or df missing.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32fb7486",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "# -------------------\n",
    "# HOG\n",
    "# -------------------\n",
    "def feat_hog_rgb(img_rgb):\n",
    "    g = (rgb2gray(img_rgb) * 255).astype('uint8')\n",
    "    fd = hog(g, orientations=9, pixels_per_cell=(16,16), cells_per_block=(2,2), visualize=False, feature_vector=True)\n",
    "    return fd\n",
    "\n",
    "# -------------------\n",
    "# LBP\n",
    "# -------------------\n",
    "def feat_lbp_rgb(img_rgb, P=8, R=1):\n",
    "    gray = (rgb2gray(img_rgb) * 255).astype('uint8')\n",
    "    lbp = local_binary_pattern(gray, P, R, method='uniform')\n",
    "    (hist, _) = np.histogram(lbp.ravel(), bins=np.arange(0, P+3), range=(0, P+2))\n",
    "    hist = hist.astype(\"float\")\n",
    "    hist /= (hist.sum() + 1e-6)\n",
    "    return hist\n",
    "\n",
    "# -------------------\n",
    "# GLCM simplifié avec NumPy (sans skimage)\n",
    "# -------------------\n",
    "def feat_glcm_rgb(img_rgb, distances=[1], angles=[0]):\n",
    "    gray = (rgb2gray(img_rgb) * 255).astype('uint8')\n",
    "    levels = 8\n",
    "    gray_q = (gray // (256 // levels)).astype('uint8')\n",
    "    glcm = np.zeros((levels, levels), dtype=float)\n",
    "    \n",
    "    # Exemple simple : angle = 0 et distance = 1\n",
    "    for i in range(gray_q.shape[0]):\n",
    "        for j in range(gray_q.shape[1]-1):\n",
    "            a = gray_q[i, j]\n",
    "            b = gray_q[i, j+1]\n",
    "            glcm[a, b] += 1\n",
    "    \n",
    "    glcm /= glcm.sum() + 1e-6  # normalisation\n",
    "    \n",
    "    # Extraire des propriétés simples\n",
    "    contrast = np.sum((np.arange(levels)[:,None]-np.arange(levels)[None,:])**2 * glcm)\n",
    "    energy = np.sum(glcm**2)\n",
    "    homogeneity = np.sum(glcm / (1. + np.abs(np.arange(levels)[:,None]-np.arange(levels)[None,:])))\n",
    "    \n",
    "    return np.array([contrast, energy, homogeneity])\n",
    "\n",
    "# -------------------\n",
    "# Moments de Hu\n",
    "# -------------------\n",
    "def feat_hu_rgb(img_rgb):\n",
    "    gray = (rgb2gray(img_rgb) * 255).astype('uint8')\n",
    "    moments = cv2.moments(gray)\n",
    "    hu = cv2.HuMoments(moments).ravel()\n",
    "    hu = -np.sign(hu) * np.log10(np.abs(hu) + 1e-6)\n",
    "    return hu\n",
    "\n",
    "# -------------------\n",
    "# Histogramme HSV\n",
    "# -------------------\n",
    "def feat_hsv_hist(img_rgb, bins=(16,16,8)):\n",
    "    hsv = cv2.cvtColor((img_rgb*255).astype('uint8'), cv2.COLOR_RGB2HSV)\n",
    "    hist = cv2.calcHist([hsv], [0,1,2], None, bins, [0,180,0,256,0,256])\n",
    "    hist = cv2.normalize(hist, hist).flatten()\n",
    "    return hist\n",
    "\n",
    "# -------------------\n",
    "# Extraction complète\n",
    "# -------------------\n",
    "def extract_classical(img_rgb):\n",
    "    feats = [\n",
    "        feat_hog_rgb(img_rgb),\n",
    "        feat_lbp_rgb(img_rgb),\n",
    "        feat_glcm_rgb(img_rgb),\n",
    "        feat_hu_rgb(img_rgb),\n",
    "        feat_hsv_hist(img_rgb)\n",
    "    ]\n",
    "    return np.concatenate([f.ravel() for f in feats])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7633fa5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "FEATURES_CACHE = Path(\"C:/Users/cheic/Documents/features/egg_classical_features.npz\")\n",
    "\n",
    "# Crée le dossier automatiquement\n",
    "FEATURES_CACHE.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def build_classical_features(df, limit=None, force=False):\n",
    "    if FEATURES_CACHE.exists() and not force:\n",
    "        print('Loading cached classical features...')\n",
    "        data = np.load(str(FEATURES_CACHE), allow_pickle=True)\n",
    "        return data['X'], data['y'], data['paths']\n",
    "    rows = df if limit is None else df.sample(limit, random_state=42)\n",
    "    X, y, paths = [], [], []\n",
    "    for _, r in tqdm(rows.iterrows(), total=len(rows)):\n",
    "        try:\n",
    "            img = preprocess_cv2(r['path'])\n",
    "            feats = extract_classical(img)\n",
    "            X.append(feats)\n",
    "            y.append(r['label_enc'])\n",
    "            paths.append(r['path'])\n",
    "        except Exception as e:\n",
    "            print('Error on', r['path'], e)\n",
    "    X = np.vstack(X)\n",
    "    y = np.array(y)\n",
    "    np.savez_compressed(str(FEATURES_CACHE), X=X, y=y, paths=np.array(paths))\n",
    "    return X, y, paths\n",
    "\n",
    "print('Call build_classical_features(df) to compute classical features (cached).')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45cf0cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_backbone(name='efficientnet_b0', pretrained=True):\n",
    "    model = timm.create_model(name, pretrained=pretrained, num_classes=0, global_pool='avg')\n",
    "    model.eval().to(device)\n",
    "    return model\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n",
    "])\n",
    "\n",
    "def extract_deep_embedding(model, img_rgb):\n",
    "    from PIL import Image\n",
    "    arr = (img_rgb*255).astype('uint8')\n",
    "    pil = Image.fromarray(arr)\n",
    "    x = transform(pil).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        feat = model(x)\n",
    "    return feat.cpu().numpy().ravel()\n",
    "\n",
    "DEEP_CACHE = Path(\"C:/Users/cheic/Documents/features/egg_deep_features.npz\")\n",
    "\n",
    "def build_deep_features(df, model_name='efficientnet_b0', limit=None, force=False):\n",
    "    if DEEP_CACHE.exists() and not force:\n",
    "        print('Loading cached deep features...')\n",
    "        data = np.load(str(DEEP_CACHE), allow_pickle=True)\n",
    "        return data['X'], data['y'], data['paths']\n",
    "    model = get_backbone(model_name)\n",
    "    rows = df if limit is None else df.sample(limit, random_state=42)\n",
    "    X, y, paths = [], [], []\n",
    "    for _, r in tqdm(rows.iterrows(), total=len(rows)):\n",
    "        try:\n",
    "            img = preprocess_cv2(r['path'])\n",
    "            emb = extract_deep_embedding(model, img)\n",
    "            X.append(emb)\n",
    "            y.append(r['label_enc'])\n",
    "            paths.append(r['path'])\n",
    "        except Exception as e:\n",
    "            print('Error on', r['path'], e)\n",
    "    X = np.vstack(X)\n",
    "    y = np.array(y)\n",
    "    np.savez_compressed(str(DEEP_CACHE), X=X, y=y, paths=np.array(paths))\n",
    "    return X, y, paths\n",
    "\n",
    "print('Deep feature builder ready. Use build_deep_features(df) to compute embeddings (cached).')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb581cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_hybrid(X_deep, X_classic):\n",
    "    return np.concatenate([X_deep, X_classic], axis=1)\n",
    "\n",
    "def reduce_pca(X, n_components=128):\n",
    "    pca = PCA(n_components=n_components, random_state=42)\n",
    "    Xr = pca.fit_transform(X)\n",
    "    return Xr, pca\n",
    "\n",
    "def build_autoencoder_encoder(input_dim, bottleneck=128):\n",
    "    try:\n",
    "        from tensorflow import keras\n",
    "        from tensorflow.keras import layers\n",
    "    except Exception as e:\n",
    "        print('TensorFlow/Keras not available:', e)\n",
    "        return None, None\n",
    "    inp = keras.Input(shape=(input_dim,))\n",
    "    x = layers.Dense(1024, activation='relu')(inp)\n",
    "    x = layers.Dense(512, activation='relu')(x)\n",
    "    bott = layers.Dense(bottleneck, activation='relu', name='bottleneck')(x)\n",
    "    x = layers.Dense(512, activation='relu')(bott)\n",
    "    x = layers.Dense(1024, activation='relu')(x)\n",
    "    out = layers.Dense(input_dim, activation='linear')(x)\n",
    "    ae = keras.Model(inp, out)\n",
    "    encoder = keras.Model(inp, bott)\n",
    "    ae.compile(optimizer='adam', loss='mse')\n",
    "    return ae, encoder\n",
    "\n",
    "print('Hybrid + reduction helpers ready.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20525e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_and_eval(X, y, model_name='svc_rbf', test_size=0.2, random_state=42):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, stratify=y, random_state=random_state)\n",
    "    scaler = StandardScaler()\n",
    "    X_train_s = scaler.fit_transform(X_train)\n",
    "    X_test_s = scaler.transform(X_test)\n",
    "    if model_name == 'svc_rbf':\n",
    "        model = SVC(kernel='rbf', probability=True, class_weight='balanced')\n",
    "    elif model_name == 'rf':\n",
    "        model = RandomForestClassifier(n_estimators=200, n_jobs=-1)\n",
    "    elif model_name == 'xgb':\n",
    "        if not XGBOOST_AVAILABLE:\n",
    "            raise RuntimeError('XGBoost not available in this environment')\n",
    "        model = xgb.XGBClassifier(n_estimators=200, use_label_encoder=False, eval_metric='logloss')\n",
    "    elif model_name == 'mlp':\n",
    "        from sklearn.neural_network import MLPClassifier\n",
    "        model = MLPClassifier(hidden_layer_sizes=(512,128), max_iter=200)\n",
    "    else:\n",
    "        model = LogisticRegression(max_iter=200, class_weight='balanced')\n",
    "\n",
    "    model.fit(X_train_s, y_train)\n",
    "    y_pred = model.predict(X_test_s)\n",
    "    report = classification_report(y_test, y_pred, output_dict=True, target_names=['good','bad'])\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    metrics = {'accuracy': accuracy_score(y_test,y_pred), 'f1_macro': f1_score(y_test,y_pred, average='macro'),\n",
    "               'recall_good': recall_score(y_test,y_pred, pos_label=0), 'recall_bad': recall_score(y_test,y_pred, pos_label=1)}\n",
    "    return {'model': model, 'scaler': scaler, 'report': report, 'cm': cm, 'metrics': metrics, 'X_test': X_test, 'y_test': y_test, 'y_pred': y_pred}\n",
    "\n",
    "def show_confusion(cm, labels=['good','bad']):\n",
    "    plt.figure(figsize=(4,4))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    plt.title('Confusion matrix')\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(labels))\n",
    "    plt.xticks(tick_marks, labels, rotation=45)\n",
    "    plt.yticks(tick_marks, labels)\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in np.ndindex(cm.shape):\n",
    "        plt.text(j, i, format(cm[i, j], 'd'), horizontalalignment=\"center\", color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.show()\n",
    "\n",
    "print('Training & evaluation helpers ready.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a56f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Exécution recommandée (décommente selon besoins)\n",
    "X_classic, y_classic, paths = build_classical_features(df, limit=None, force=False)\n",
    "X_deep, y_deep, paths = build_deep_features(df, model_name='efficientnet_b0', limit=None, force=False)\n",
    "X_hybrid = build_hybrid(X_deep, X_classic)\n",
    "Xr, pca = reduce_pca(X_hybrid, n_components=128)\n",
    "res = train_and_eval(Xr, y_classic, model_name='svc_rbf')\n",
    "print(res['metrics'])\n",
    "show_confusion(res['cm'])\n",
    "print('Cells ready - follow instructions in this notebook to run experiments.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1018a3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train score:\", model.score(X_train, y_train))\n",
    "print(\"Test score:\", model.score(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00d8dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def save_pipeline(model, scaler, out_path='/mnt/data/experiment_pipeline'):\n",
    "    os.makedirs(out_path, exist_ok=True)\n",
    "    joblib.dump({'model': model, 'scaler': scaler}, os.path.join(out_path, 'pipeline.pkl'))\n",
    "    print('Saved pipeline to', out_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
